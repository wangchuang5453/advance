<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/spark-md5@3.0.2/spark-md5.min.js"></script>
    <title>upload</title>
</head>
<body>
    <div>
      <input type="file" id="upload">
      <button id="submit">上传</button>
    </div>
    <script>
      
      document.getElementById('submit').addEventListener('click', (e) => {
        
      })

      async function uploadFile() {
        // 获取文件
        const file = document.getElementById('upload').files[0];
        // 计算文件的 MD5
        const fileMd5 = await calcFileMD5(file);
        // 发送请求给后台，判断文件是否已经存在
        const fileStatus = await checkFileExist(
          "/exists", 
          file.name, fileMd5
        );
        // 如果文件已经上传了，就秒传过去，做个假像
        if (fileStatus.data && fileStatus.data.isExists) {
          console.log("文件已上传[秒传]");
          return;
        }
        // 
        await upload({
          url: "/single",
          file, // 文件对象
          fileMd5, // 文件MD5值
          fileSize: file.size, // 文件大小
          chunkSize: 1 * 1024 * 1024, // 分块大小
          chunkIds: fileStatus.data.chunkIds, // 已上传的分块列表
          poolLimit: 3, // 限制的并发数
        });
      }

      function calcFileMD5(file) {
        return new Promise((resolve, reject) => {
          /**
           * https://www.npmjs.com/package/spark-md5 示例代码
          */
          let blobSlice = File.prototype.slice || File.prototype.mozSlice || File.prototype.webkitSlice,
              chunkSize = 2097152, // Read in chunks of 2MB
              chunks = Math.ceil(file.size / chunkSize),
              currentChunk = 0,
              spark = new SparkMD5.ArrayBuffer(),
              fileReader = new FileReader();

          fileReader.onload = (e) => {
            console.log('read chunk nr', currentChunk + 1, 'of', chunks);
            spark.append(e.target.result); // Append array buffer
            currentChunk++;

            if (currentChunk < chunks) {
              loadNext();
            } else {
              resolve(spark.end());
            }
          };

          fileReader.onerror = (e) => {
            reject(fileReader.error);
            spark.abort();
          };

          function loadNext() {
            let start = currentChunk * chunkSize,
                end = ((start + chunkSize) >= file.size) ? file.size : start + chunkSize;

            fileReader.readAsArrayBuffer(blobSlice.call(file, start, end));
          }

          loadNext();
        });
      }

      function checkFileExist() {
        // 
      }
      async function asyncPool(poolLimit, array, iteratorFn) {
        // 
      }
    </script>
</body>
</html>